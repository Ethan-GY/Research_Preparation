{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a482108-b113-4dfe-8f2a-be48ac4a4e67",
   "metadata": {},
   "source": [
    "# 多模态核心与论文精读 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8b5160-4fa9-4345-bac0-221c04ce27cc",
   "metadata": {},
   "source": [
    "> 深入理解Transformer架构和当前主流多模态模型，具备精读和复现论文核心代码的能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a678dd00-6ca1-4a55-b52b-7d298ec90081",
   "metadata": {},
   "source": [
    "## Part1:Transformer架构基石 - Attention与Positional Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ff9d1e9-641e-478b-8934-795f93ed4672",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention机制: 理解Scaled Dot-Product Attention的计算过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c66aaac-0665-46cf-8582-d9c77007ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer架构: Encoder-Decoder结构，Positional Encoding的作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c5203ea-ea64-4322-a54b-0b654444571a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n使用PyTorch手动实现一个简化版的Self-Attention层和Positional Encoding。在一个小的toy数据集上验证其基本功能。\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "使用PyTorch手动实现一个简化版的Self-Attention层和Positional Encoding。在一个小的toy数据集上验证其基本功能。\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09df2f3d-ac9c-4d7b-b3fd-45dbd427ba2b",
   "metadata": {},
   "source": [
    "## Part2:多模态开山之作 - CLIP论文精读与复现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5efaf2d7-8839-481e-842f-3a91d1f4d297",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLIP论文: 理解对比学习（Contrastive Learning）思想，图文对（Image-Text Pair）的构建，Zero-shot分类原理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de7f57f6-fb9b-46a9-97bc-a1d031fadca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n使用Hugging Face的 transformers 库，加载预训练的CLIP模型 (openai/clip-vit-base-patch32)。\\n复现论文中的核心功能：计算一组图片和一组文本描述之间的相似度矩阵，并找出每张图片最匹配的文本描述。\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "使用Hugging Face的 transformers 库，加载预训练的CLIP模型 (openai/clip-vit-base-patch32)。\n",
    "复现论文中的核心功能：计算一组图片和一组文本描述之间的相似度矩阵，并找出每张图片最匹配的文本描述。\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c89334-99d9-4089-9585-e80bd4c49037",
   "metadata": {},
   "source": [
    "## Part3:多模态全能选手 - BLIP论文精读\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc595501-4eb2-4322-8305-53c639f3cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLIP论文: 理解其如何统一图像-文本的理解（如VQA）和生成（如Image Captioning）任务，Bootstrapping数据的构建方法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15907b86-be0b-4b16-858d-082f367b1e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n阅读BLIP论文，撰写一篇精读笔记，重点分析其与CLIP的异同点、创新之处以及适用场景。尝试使用Hugging Face加载BLIP模型，对一张图片进行caption生成。\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "阅读BLIP论文，撰写一篇精读笔记，重点分析其与CLIP的异同点、创新之处以及适用场景。尝试使用Hugging Face加载BLIP模型，对一张图片进行caption生成。\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df82027-6b9f-4ccd-b1d6-710caadecb8b",
   "metadata": {},
   "source": [
    "## Part4:视觉语言对话 - LLaVA初步探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63acf4d3-5b1d-447a-a1e5-53eeee426654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLaVA: 理解其架构（视觉编码器 + LLM），如何将图像信息注入到语言模型中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06467ab0-3c0b-4814-a0bd-ecaed52f0ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "在本地或Colab上，使用LLaVA的官方Demo或Hugging Face Space，与模型进行多轮视觉问答（VQA）交互。记录下模型表现优秀和不足的案例，并尝试分析原因。\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "mllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
